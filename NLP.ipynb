{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('News.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukraine\n",
      "Ukraine\n",
      "Kherson\n",
      "Kherson\n",
      "Russia\n",
      "Ukraine\n",
      "Russia\n",
      "Ukraine\n",
      "London\n",
      "Russia\n",
      "the west bank\n",
      "Ukrainian\n",
      "Kherson\n",
      "Kakhova\n",
      "Ukraine\n",
      "Kherson\n",
      "Zaporizhzhia\n",
      "Ukraine\n",
      "Moscow\n",
      "US\n",
      "Ukraine\n",
      "agoGermany\n",
      "Israel\n",
      "Ukraine\n",
      "Dagestan\n",
      "UK\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(lines)\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    if ent.label_ == \"GPE\" :\n",
    "        print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Ukraine,\n",
       " Russian,\n",
       " Dnipro River,\n",
       " Ukraine,\n",
       " The Guardian\n",
       " Skip,\n",
       " Guardian - Back,\n",
       " NewsWorld,\n",
       " crisisUkraineEnvironmentScienceGlobal,\n",
       " OpinionThe Guardian,\n",
       " CultureBooksMusicTV & radioArt & designFilmGamesClassicalStageLifestyleView,\n",
       " & sexHealth & fitnessHome & gardenWomenMenFamilyTravelMoneySearch,\n",
       " SearchSupport,\n",
       " ArchiveGuardian Puzzles,\n",
       " LicensingThe Guardian,\n",
       " appVideoPodcastsPicturesNewslettersToday,\n",
       " the GuardianThe ObserverGuardian,\n",
       " ArchiveGuardian Puzzles,\n",
       " Ukrainian,\n",
       " the Dnipro River,\n",
       " Kherson,\n",
       " last month,\n",
       " Alex Babenko/,\n",
       " the Dnipro River,\n",
       " Kherson,\n",
       " last month,\n",
       " Alex Babenko,\n",
       " Russian,\n",
       " Dnipro RiverBeachhead,\n",
       " Kherson,\n",
       " Russia,\n",
       " Tom Burgis,\n",
       " KhersonFri,\n",
       " GMTLast,\n",
       " Fri 3 Nov 2023 20.40,\n",
       " Ukrainian,\n",
       " the Dnipro River,\n",
       " Russian,\n",
       " Ukraine,\n",
       " Kherson,\n",
       " Ukrainian,\n",
       " Crimea,\n",
       " Russia,\n",
       " 2014,\n",
       " two,\n",
       " Ukrainian,\n",
       " the east bank,\n",
       " mid-October,\n",
       " Russian,\n",
       " Ukrainian,\n",
       " Volodymyr Saldo,\n",
       " Putin,\n",
       " Kherson,\n",
       " Russian,\n",
       " Ukrainians,\n",
       " Kherson,\n",
       " Ukraine,\n",
       " Russian,\n",
       " Russian,\n",
       " fortificationsThis week,\n",
       " Rybar,\n",
       " one,\n",
       " Russian,\n",
       " night,\n",
       " Ukrainian,\n",
       " Ukrainian,\n",
       " thousand-kilometre,\n",
       " Russians,\n",
       " Ben Barry,\n",
       " British,\n",
       " the International Institute for Strategic Studies,\n",
       " London,\n",
       " Russian,\n",
       " Ukrainian,\n",
       " the east bank,\n",
       " last year,\n",
       " Kherson,\n",
       " the Dnipro River,\n",
       " Russia,\n",
       " the first days,\n",
       " February 2022,\n",
       " Ukrainian,\n",
       " Kherson,\n",
       " the west bank,\n",
       " Kherson,\n",
       " Russian,\n",
       " almost every day,\n",
       " Ukrainian,\n",
       " between a few dozen and a few hundred,\n",
       " Krynky,\n",
       " Ukrainian,\n",
       " Ostap Vishnia,\n",
       " Ukrainian,\n",
       " Russian,\n",
       " Ukrainian,\n",
       " the Dnipro River,\n",
       " Kherson,\n",
       " Alex Babenko,\n",
       " Kakhova,\n",
       " Dnipro,\n",
       " June,\n",
       " Russians,\n",
       " Kremlin,\n",
       " One,\n",
       " Ukraine,\n",
       " Russian,\n",
       " Ukrainians,\n",
       " Beyond Krynky,\n",
       " Russian,\n",
       " 500 metres,\n",
       " Putin,\n",
       " February 2022,\n",
       " Kherson,\n",
       " Zaporizhzhia,\n",
       " Ukraine,\n",
       " Ukrainian,\n",
       " the east bank,\n",
       " Moscow,\n",
       " Russian,\n",
       " Col Gen,\n",
       " Oleg Makarevich,\n",
       " the Institute for the Study of War,\n",
       " US,\n",
       " Ukraine,\n",
       " Makarevich’s,\n",
       " Kremlin,\n",
       " Russian,\n",
       " Ukrainian,\n",
       " agoRussia-Ukraine,\n",
       " day 61816h,\n",
       " agoUkraine,\n",
       " Russian,\n",
       " agoGermany,\n",
       " EU,\n",
       " Biden,\n",
       " Congress,\n",
       " Israel,\n",
       " nine,\n",
       " Ukraine,\n",
       " Italian,\n",
       " Putin,\n",
       " Dagestan,\n",
       " Guardian,\n",
       " emailHelpComplaints & correctionsSecureDropWork,\n",
       " Privacy policyCookie policyTerms & conditionsContact,\n",
       " UK,\n",
       " 2023,\n",
       " Guardian News & Media Limited,\n",
       " dcr)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5912129800317639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sures\\AppData\\Local\\Temp\\ipykernel_29416\\1013727239.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc2.similarity(doc3))\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"the Israel-Hamas war\")\n",
    "doc3 = nlp(\"Anthony Albanese’s three-day tour of Shanghai and Beijing\")\n",
    "\n",
    "print(doc2.similarity(doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      ":\n",
      "based\n",
      "on\n",
      "the\n",
      "keywords\n",
      "in\n",
      "the\n",
      "title\n",
      "and\n",
      "body\n",
      "of\n",
      "the\n",
      "text\n",
      ",\n",
      "i\n",
      "would\n",
      "categorize\n",
      "the\n",
      "article\n",
      "as\n",
      "belonging\n",
      "to\n",
      "the\n",
      "\"\n",
      "politics\n",
      "\"\n",
      "class\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "guardian\n",
      "is\n",
      "a\n",
      "uk\n",
      "newspaper\n",
      "and\n",
      "they\n",
      "are\n",
      "using\n",
      "their\n",
      "own\n",
      "category\n",
      "scheme\n",
      "which\n",
      "is\n",
      "different\n",
      "from\n",
      "the\n",
      "one\n",
      "used\n",
      "by\n",
      "the\n",
      "user\n",
      ".\n",
      "the\n",
      "user\n",
      "has\n",
      "categorized\n",
      "the\n",
      "article\n",
      "under\n",
      "\"\n",
      "politics\n",
      "\"\n",
      "but\n",
      "the\n",
      "guardian\n",
      "has\n",
      "categorized\n",
      "it\n",
      "under\n",
      "\"\n",
      "israel\n",
      "\"\n",
      ".\n",
      "so\n",
      "there\n",
      "is\n",
      "a\n",
      "difference\n",
      "in\n",
      "how\n",
      "the\n",
      "article\n",
      "is\n",
      "being\n",
      "categorized\n",
      "by\n",
      "the\n",
      "two\n",
      "entities\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "doc4 = \"\"\"Computer: Based on the keywords in the title and body of the text, I would categorize the article as belonging to the \"Politics\" class.\n",
    "\n",
    "The Guardian is a UK newspaper and they are using their own category scheme which is different from the one used by the user. The user has categorized the article under \"Politics\" but the Guardian has categorized it under \"Israel\". So there is a difference in how the article is being categorized by the two entities.. \"\"\"\n",
    "\n",
    "for tok in nlp(doc4):\n",
    "    print(str(tok).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classify to broad dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built a multilabel classification system using SpaCy (build and train the modek)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(train_data):\n",
    "\n",
    "    train_data = train_data.drop(columns=[\"Id\"])\n",
    "\n",
    "    data = tuple(zip(train_data['Title'].tolist(), train_data['Label'].tolist())) \n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        if (label=='Nature'):\n",
    "            doc.cats['Nature'] = 1\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Politics'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 1\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Entertainment'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 1\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Economics'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 1\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Culture'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 1\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Science'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 1        \n",
    "\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs,train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 372.32it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 512.69it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\Work\\Programming\\Hackathons\\Diplomacy software\\Dimensions.csv\")\n",
    "train = df.sample(frac=0.7, replace=False, random_state=1)\n",
    "\n",
    "train_docs, train_data = make_docs(train)\n",
    "\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./textcat_data/textcat_train.spacy\")\n",
    "\n",
    "test = df.sample(frac=0.3, replace=False, random_state=1)\n",
    "\n",
    "test_docs, train_data = make_docs(test)\n",
    "\n",
    "doc_bin = DocBin(docs=test_docs)\n",
    "doc_bin.to_disk(\"./textcat_data/textcat_valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NBA Playoffs: Lakers and Clippers Face Off in ...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Exotic Destinations for Your Next Adventure Va...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>President Signs Historic Climate Change Legisl...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Scientific Expedition Discovers New Species in...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>High School Robotics Team Wins International C...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mental Health Awareness Month: Initiatives to ...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Innovative Schools Implement Technology-Driven...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Wildfires Threaten Communities in the Western ...</td>\n",
       "      <td>Nature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Researchers Develop Promising Treatment for Al...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Education Policy Reforms Aim to Improve Studen...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Discovering the Hidden Gems of a Charming Euro...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Breakthrough in Quantum Computing Paves the Wa...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title          Label\n",
       "2   NBA Playoffs: Lakers and Clippers Face Off in ...  Entertainment\n",
       "30  Exotic Destinations for Your Next Adventure Va...  Entertainment\n",
       "3   President Signs Historic Climate Change Legisl...       Politics\n",
       "21  Scientific Expedition Discovers New Species in...        Science\n",
       "26  High School Robotics Team Wins International C...       Politics\n",
       "28  Mental Health Awareness Month: Initiatives to ...        Science\n",
       "22  Innovative Schools Implement Technology-Driven...        Science\n",
       "36  Wildfires Threaten Communities in the Western ...         Nature\n",
       "19  Researchers Develop Promising Treatment for Al...        Science\n",
       "25  Education Policy Reforms Aim to Improve Studen...       Politics\n",
       "31  Discovering the Hidden Gems of a Charming Euro...  Entertainment\n",
       "17  Breakthrough in Quantum Computing Paves the Wa...        Science"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "textcat_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train textcat_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config ./textcat_base_config.cfg ./textcat_config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-27 22:02:30,992] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "[2023-10-27 22:02:31,180] [INFO] Set up nlp object from config\n",
      "[2023-10-27 22:02:31,187] [DEBUG] Loading corpus from path: textcat_data\\textcat_valid.spacy\n",
      "[2023-10-27 22:02:31,192] [DEBUG] Loading corpus from path: textcat_data\\textcat_train.spacy\n",
      "[2023-10-27 22:02:31,192] [INFO] Pipeline: ['tok2vec', 'textcat']\n",
      "[2023-10-27 22:02:31,192] [INFO] Created vocabulary\n",
      "[2023-10-27 22:02:31,587] [WARNING] [W112] The model specified to use for initial vectors (en_core_web_sm) has no vectors. This is almost certainly a mistake.\n",
      "[2023-10-27 22:02:31,587] [INFO] Added vectors: en_core_web_sm\n",
      "[2023-10-27 22:02:31,587] [INFO] Finished initializing nlp object\n",
      "[2023-10-27 22:02:31,852] [INFO] Initialized pipeline components: ['tok2vec', 'textcat']\n",
      "[2023-10-27 22:02:31,864] [DEBUG] Loading corpus from path: textcat_data\\textcat_valid.spacy\n",
      "[2023-10-27 22:02:31,864] [DEBUG] Loading corpus from path: textcat_data\\textcat_train.spacy\n",
      "[2023-10-27 22:02:31,872] [DEBUG] Removed existing output directory: textcat_output\\model-best\n",
      "[2023-10-27 22:02:31,882] [DEBUG] Removed existing output directory: textcat_output\\model-last\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: textcat_output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ------------  ----------  ------\n",
      "  0       0          0.00          0.14       37.26    0.37\n",
      " 77     200         84.36          1.09       66.67    0.67\n",
      "177     400        482.64          0.23       66.67    0.67\n",
      "277     600         19.61          0.01       66.67    0.67\n",
      "418     800          0.00          0.00       66.67    0.67\n",
      "618    1000          0.00          0.00       66.67    0.67\n",
      "818    1200          0.00          0.00       66.67    0.67\n",
      "1018    1400          0.00          0.00       66.67    0.67\n",
      "1218    1600          0.00          0.00       66.67    0.67\n",
      "1418    1800          0.00          0.00       66.67    0.67\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "textcat_output\\model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train textcat_config.cfg --verbose --output ./textcat_output --paths.train textcat_data/textcat_train.spacy --paths.dev textcat_data/textcat_valid.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nature': 0.6959365010261536, 'Politics': 2.370913716731593e-05, 'Entertainment': 2.5340261000650344e-09, 'Economics': 0.1244477927684784, 'Culture': 0.00027904147282242775, 'Science': 0.17931297421455383}\n",
      "Singapore votes in favour of UN resolution to protect civilians, uphold humanitarian obligations in Gaza Strip\n",
      "The class of this headline is Nature\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Enter a headline to analyse: \")\n",
    "nlp_textcat = spacy.load(os.getcwd() + \"\\\\textcat_output\\\\model-last\")\n",
    "docPred = nlp_textcat(prompt)\n",
    "res = docPred.cats\n",
    "print(res)\n",
    "print(prompt)\n",
    "print(\"The class of this headline is\", max(res, key=res.get))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Map onto seriousness of risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Differentiate domesting and international events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ukraine', 'Ukraine', 'Kherson', 'Kherson', 'Russia', 'Ukraine', 'Russia', 'Ukraine', 'London', 'Russia', 'the west bank', 'Ukrainian', 'Kherson', 'Kakhova', 'Ukraine', 'Kherson', 'Zaporizhzhia', 'Ukraine', 'Moscow', 'US', 'Ukraine', 'agoGermany', 'Israel', 'Ukraine', 'Dagestan', 'UK']\n"
     ]
    }
   ],
   "source": [
    "with open('News.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.read()\n",
    "\n",
    "doc1 = nlp(lines)\n",
    "\n",
    "places = []\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    if ent.label_ == \"GPE\" :\n",
    "        places.append(ent.text)\n",
    "\n",
    "print(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This blocks initialised the geography module\n",
    "from functools import partial\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"diplomacy-software\")\n",
    "\n",
    "geocode = partial(geolocator.geocode, language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ukraine': 14,\n",
       " 'Russia': 5,\n",
       " 'United Kingdom': 1,\n",
       " 'United States': 1,\n",
       " 'Czechia': 1,\n",
       " 'Israel': 1,\n",
       " 'Central African Republic': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = {}\n",
    "\n",
    "for place in places:\n",
    "    try:\n",
    "        country = geocode(place)[0].split(',')[-1].strip()\n",
    "\n",
    "        if country not in countries:\n",
    "            countries[country] = 1\n",
    "        else:\n",
    "            countries[country] += 1\n",
    "\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ukraine', 'Russia']\n"
     ]
    }
   ],
   "source": [
    "total = sum(list(countries.values()))\n",
    "\n",
    "threshold = total / len(countries)\n",
    "\n",
    "relevant = []\n",
    "\n",
    "for country in countries:\n",
    "    countries[country] = countries[country] / total\n",
    "\n",
    "    if countries[country] > threshold:\n",
    "        relevant.append(country)\n",
    "\n",
    "print(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
