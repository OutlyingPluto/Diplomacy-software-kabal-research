{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('News.txt', encoding=\"utf8\") as f:\n",
    "    lines = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukraine\n",
      "Ukraine\n",
      "Kherson\n",
      "Kherson\n",
      "Russia\n",
      "Ukraine\n",
      "Russia\n",
      "Ukraine\n",
      "London\n",
      "Russia\n",
      "the west bank\n",
      "Ukrainian\n",
      "Kherson\n",
      "Kakhova\n",
      "Ukraine\n",
      "Kherson\n",
      "Zaporizhzhia\n",
      "Ukraine\n",
      "Moscow\n",
      "US\n",
      "Ukraine\n",
      "agoGermany\n",
      "Israel\n",
      "Ukraine\n",
      "Dagestan\n",
      "UK\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(lines)\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    if ent.label_ == \"GPE\" :\n",
    "        print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Ukraine,\n",
       " Russian,\n",
       " Dnipro River,\n",
       " Ukraine,\n",
       " The Guardian\n",
       " Skip,\n",
       " Guardian - Back,\n",
       " NewsWorld,\n",
       " crisisUkraineEnvironmentScienceGlobal,\n",
       " OpinionThe Guardian,\n",
       " CultureBooksMusicTV & radioArt & designFilmGamesClassicalStageLifestyleView,\n",
       " & sexHealth & fitnessHome & gardenWomenMenFamilyTravelMoneySearch,\n",
       " SearchSupport,\n",
       " ArchiveGuardian Puzzles,\n",
       " LicensingThe Guardian,\n",
       " appVideoPodcastsPicturesNewslettersToday,\n",
       " the GuardianThe ObserverGuardian,\n",
       " ArchiveGuardian Puzzles,\n",
       " Ukrainian,\n",
       " the Dnipro River,\n",
       " Kherson,\n",
       " last month,\n",
       " Alex Babenko/,\n",
       " the Dnipro River,\n",
       " Kherson,\n",
       " last month,\n",
       " Alex Babenko,\n",
       " Russian,\n",
       " Dnipro RiverBeachhead,\n",
       " Kherson,\n",
       " Russia,\n",
       " Tom Burgis,\n",
       " KhersonFri,\n",
       " GMTLast,\n",
       " Fri 3 Nov 2023 20.40,\n",
       " Ukrainian,\n",
       " the Dnipro River,\n",
       " Russian,\n",
       " Ukraine,\n",
       " Kherson,\n",
       " Ukrainian,\n",
       " Crimea,\n",
       " Russia,\n",
       " 2014,\n",
       " two,\n",
       " Ukrainian,\n",
       " the east bank,\n",
       " mid-October,\n",
       " Russian,\n",
       " Ukrainian,\n",
       " Volodymyr Saldo,\n",
       " Putin,\n",
       " Kherson,\n",
       " Russian,\n",
       " Ukrainians,\n",
       " Kherson,\n",
       " Ukraine,\n",
       " Russian,\n",
       " Russian,\n",
       " fortificationsThis week,\n",
       " Rybar,\n",
       " one,\n",
       " Russian,\n",
       " night,\n",
       " Ukrainian,\n",
       " Ukrainian,\n",
       " thousand-kilometre,\n",
       " Russians,\n",
       " Ben Barry,\n",
       " British,\n",
       " the International Institute for Strategic Studies,\n",
       " London,\n",
       " Russian,\n",
       " Ukrainian,\n",
       " the east bank,\n",
       " last year,\n",
       " Kherson,\n",
       " the Dnipro River,\n",
       " Russia,\n",
       " the first days,\n",
       " February 2022,\n",
       " Ukrainian,\n",
       " Kherson,\n",
       " the west bank,\n",
       " Kherson,\n",
       " Russian,\n",
       " almost every day,\n",
       " Ukrainian,\n",
       " between a few dozen and a few hundred,\n",
       " Krynky,\n",
       " Ukrainian,\n",
       " Ostap Vishnia,\n",
       " Ukrainian,\n",
       " Russian,\n",
       " Ukrainian,\n",
       " the Dnipro River,\n",
       " Kherson,\n",
       " Alex Babenko,\n",
       " Kakhova,\n",
       " Dnipro,\n",
       " June,\n",
       " Russians,\n",
       " Kremlin,\n",
       " One,\n",
       " Ukraine,\n",
       " Russian,\n",
       " Ukrainians,\n",
       " Beyond Krynky,\n",
       " Russian,\n",
       " 500 metres,\n",
       " Putin,\n",
       " February 2022,\n",
       " Kherson,\n",
       " Zaporizhzhia,\n",
       " Ukraine,\n",
       " Ukrainian,\n",
       " the east bank,\n",
       " Moscow,\n",
       " Russian,\n",
       " Col Gen,\n",
       " Oleg Makarevich,\n",
       " the Institute for the Study of War,\n",
       " US,\n",
       " Ukraine,\n",
       " Makarevich’s,\n",
       " Kremlin,\n",
       " Russian,\n",
       " Ukrainian,\n",
       " agoRussia-Ukraine,\n",
       " day 61816h,\n",
       " agoUkraine,\n",
       " Russian,\n",
       " agoGermany,\n",
       " EU,\n",
       " Biden,\n",
       " Congress,\n",
       " Israel,\n",
       " nine,\n",
       " Ukraine,\n",
       " Italian,\n",
       " Putin,\n",
       " Dagestan,\n",
       " Guardian,\n",
       " emailHelpComplaints & correctionsSecureDropWork,\n",
       " Privacy policyCookie policyTerms & conditionsContact,\n",
       " UK,\n",
       " 2023,\n",
       " Guardian News & Media Limited,\n",
       " dcr)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5912129800317639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sures\\AppData\\Local\\Temp\\ipykernel_29416\\1013727239.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(doc2.similarity(doc3))\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"the Israel-Hamas war\")\n",
    "doc3 = nlp(\"Anthony Albanese’s three-day tour of Shanghai and Beijing\")\n",
    "\n",
    "print(doc2.similarity(doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      ":\n",
      "based\n",
      "on\n",
      "the\n",
      "keywords\n",
      "in\n",
      "the\n",
      "title\n",
      "and\n",
      "body\n",
      "of\n",
      "the\n",
      "text\n",
      ",\n",
      "i\n",
      "would\n",
      "categorize\n",
      "the\n",
      "article\n",
      "as\n",
      "belonging\n",
      "to\n",
      "the\n",
      "\"\n",
      "politics\n",
      "\"\n",
      "class\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "the\n",
      "guardian\n",
      "is\n",
      "a\n",
      "uk\n",
      "newspaper\n",
      "and\n",
      "they\n",
      "are\n",
      "using\n",
      "their\n",
      "own\n",
      "category\n",
      "scheme\n",
      "which\n",
      "is\n",
      "different\n",
      "from\n",
      "the\n",
      "one\n",
      "used\n",
      "by\n",
      "the\n",
      "user\n",
      ".\n",
      "the\n",
      "user\n",
      "has\n",
      "categorized\n",
      "the\n",
      "article\n",
      "under\n",
      "\"\n",
      "politics\n",
      "\"\n",
      "but\n",
      "the\n",
      "guardian\n",
      "has\n",
      "categorized\n",
      "it\n",
      "under\n",
      "\"\n",
      "israel\n",
      "\"\n",
      ".\n",
      "so\n",
      "there\n",
      "is\n",
      "a\n",
      "difference\n",
      "in\n",
      "how\n",
      "the\n",
      "article\n",
      "is\n",
      "being\n",
      "categorized\n",
      "by\n",
      "the\n",
      "two\n",
      "entities\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "doc4 = \"\"\"Computer: Based on the keywords in the title and body of the text, I would categorize the article as belonging to the \"Politics\" class.\n",
    "\n",
    "The Guardian is a UK newspaper and they are using their own category scheme which is different from the one used by the user. The user has categorized the article under \"Politics\" but the Guardian has categorized it under \"Israel\". So there is a difference in how the article is being categorized by the two entities.. \"\"\"\n",
    "\n",
    "for tok in nlp(doc4):\n",
    "    print(str(tok).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classify to broad dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built a multilabel classification system using SpaCy (build and train the modek)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(train_data):\n",
    "\n",
    "    train_data = train_data.drop(columns=[\"Id\"])\n",
    "\n",
    "    data = tuple(zip(train_data['Title'].tolist(), train_data['Label'].tolist())) \n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    for doc, label in tqdm(nlp.pipe(data, as_tuples=True), total = len(data)):\n",
    "        if (label=='Nature'):\n",
    "            doc.cats['Nature'] = 1\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Politics'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 1\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Entertainment'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 1\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Economics'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 1\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Culture'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 1\n",
    "            doc.cats['Science']  = 0\n",
    "        elif (label=='Science'):\n",
    "            doc.cats['Nature'] = 0\n",
    "            doc.cats['Politics'] = 0\n",
    "            doc.cats['Entertainment']  = 0\n",
    "            doc.cats['Economics']  = 0\n",
    "            doc.cats['Culture']  = 0\n",
    "            doc.cats['Science']  = 1        \n",
    "\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs,train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 372.32it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 512.69it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\Work\\Programming\\Hackathons\\Diplomacy software\\Dimensions.csv\")\n",
    "train = df.sample(frac=0.7, replace=False, random_state=1)\n",
    "\n",
    "train_docs, train_data = make_docs(train)\n",
    "\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./textcat_data/textcat_train.spacy\")\n",
    "\n",
    "test = df.sample(frac=0.3, replace=False, random_state=1)\n",
    "\n",
    "test_docs, train_data = make_docs(test)\n",
    "\n",
    "doc_bin = DocBin(docs=test_docs)\n",
    "doc_bin.to_disk(\"./textcat_data/textcat_valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NBA Playoffs: Lakers and Clippers Face Off in ...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Exotic Destinations for Your Next Adventure Va...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>President Signs Historic Climate Change Legisl...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Scientific Expedition Discovers New Species in...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>High School Robotics Team Wins International C...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mental Health Awareness Month: Initiatives to ...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Innovative Schools Implement Technology-Driven...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Wildfires Threaten Communities in the Western ...</td>\n",
       "      <td>Nature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Researchers Develop Promising Treatment for Al...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Education Policy Reforms Aim to Improve Studen...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Discovering the Hidden Gems of a Charming Euro...</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Breakthrough in Quantum Computing Paves the Wa...</td>\n",
       "      <td>Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title          Label\n",
       "2   NBA Playoffs: Lakers and Clippers Face Off in ...  Entertainment\n",
       "30  Exotic Destinations for Your Next Adventure Va...  Entertainment\n",
       "3   President Signs Historic Climate Change Legisl...       Politics\n",
       "21  Scientific Expedition Discovers New Species in...        Science\n",
       "26  High School Robotics Team Wins International C...       Politics\n",
       "28  Mental Health Awareness Month: Initiatives to ...        Science\n",
       "22  Innovative Schools Implement Technology-Driven...        Science\n",
       "36  Wildfires Threaten Communities in the Western ...         Nature\n",
       "19  Researchers Develop Promising Treatment for Al...        Science\n",
       "25  Education Policy Reforms Aim to Improve Studen...       Politics\n",
       "31  Discovering the Hidden Gems of a Charming Euro...  Entertainment\n",
       "17  Breakthrough in Quantum Computing Paves the Wa...        Science"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "textcat_config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train textcat_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config ./textcat_base_config.cfg ./textcat_config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-27 22:02:30,992] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "[2023-10-27 22:02:31,180] [INFO] Set up nlp object from config\n",
      "[2023-10-27 22:02:31,187] [DEBUG] Loading corpus from path: textcat_data\\textcat_valid.spacy\n",
      "[2023-10-27 22:02:31,192] [DEBUG] Loading corpus from path: textcat_data\\textcat_train.spacy\n",
      "[2023-10-27 22:02:31,192] [INFO] Pipeline: ['tok2vec', 'textcat']\n",
      "[2023-10-27 22:02:31,192] [INFO] Created vocabulary\n",
      "[2023-10-27 22:02:31,587] [WARNING] [W112] The model specified to use for initial vectors (en_core_web_sm) has no vectors. This is almost certainly a mistake.\n",
      "[2023-10-27 22:02:31,587] [INFO] Added vectors: en_core_web_sm\n",
      "[2023-10-27 22:02:31,587] [INFO] Finished initializing nlp object\n",
      "[2023-10-27 22:02:31,852] [INFO] Initialized pipeline components: ['tok2vec', 'textcat']\n",
      "[2023-10-27 22:02:31,864] [DEBUG] Loading corpus from path: textcat_data\\textcat_valid.spacy\n",
      "[2023-10-27 22:02:31,864] [DEBUG] Loading corpus from path: textcat_data\\textcat_train.spacy\n",
      "[2023-10-27 22:02:31,872] [DEBUG] Removed existing output directory: textcat_output\\model-best\n",
      "[2023-10-27 22:02:31,882] [DEBUG] Removed existing output directory: textcat_output\\model-last\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: textcat_output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ------------  ----------  ------\n",
      "  0       0          0.00          0.14       37.26    0.37\n",
      " 77     200         84.36          1.09       66.67    0.67\n",
      "177     400        482.64          0.23       66.67    0.67\n",
      "277     600         19.61          0.01       66.67    0.67\n",
      "418     800          0.00          0.00       66.67    0.67\n",
      "618    1000          0.00          0.00       66.67    0.67\n",
      "818    1200          0.00          0.00       66.67    0.67\n",
      "1018    1400          0.00          0.00       66.67    0.67\n",
      "1218    1600          0.00          0.00       66.67    0.67\n",
      "1418    1800          0.00          0.00       66.67    0.67\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "textcat_output\\model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train textcat_config.cfg --verbose --output ./textcat_output --paths.train textcat_data/textcat_train.spacy --paths.dev textcat_data/textcat_valid.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Nature': 0.6959365010261536, 'Politics': 2.370913716731593e-05, 'Entertainment': 2.5340261000650344e-09, 'Economics': 0.1244477927684784, 'Culture': 0.00027904147282242775, 'Science': 0.17931297421455383}\n",
      "Singapore votes in favour of UN resolution to protect civilians, uphold humanitarian obligations in Gaza Strip\n",
      "The class of this headline is Nature\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Enter a headline to analyse: \")\n",
    "nlp_textcat = spacy.load(os.getcwd() + \"\\\\textcat_output\\\\model-last\")\n",
    "docPred = nlp_textcat(prompt)\n",
    "res = docPred.cats\n",
    "print(res)\n",
    "print(prompt)\n",
    "print(\"The class of this headline is\", max(res, key=res.get))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Map onto seriousness of risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Differentiate domesting and international events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = 'Imagine, if you will, the daily routine of a tortoise in winter: every day you wake up under the dog’s bed and set off on a series of clockwise laps around a large kitchen table.\\nYour visual field is wide but not very high – if the known universe has a ceiling, you have never seen it. The first three laps are flat and featureless, perfect repetitions of a cycle. But on the fourth lap half an apple suddenly appears, lying cut side up. How serendipitous! You think: I should probably eat that while I’m here, for who knows when I will next come across another … Wait, is that a grape rolling by in the distance?\\nI’m sitting at the kitchen table, drinking coffee, reading headlines and listening to the rhythmic thunk of the tortoise circling, followed by a pause when he finds the apple I left in his path, then more thunking, and then another pause for the grape.\\nAfter a few moments of silence I experience an unpleasant but familiar sensation: a spreading pool of tortoise piss reaching the edge of my sock.\\n“Ugh,” I say, lifting my foot.\\n“Fucking tortoise,” my wife says, entering the room, scooping him up and putting him outside the back door.\\n“It’s too cold,” I say.\\n“It’s not too cold,” she says. “It’s sunny.”\\n“But it will be too cold when the sun goes down,” I say, “and then I’ll have to find him in the dark.”\\n“That’s right,” my wife says. “You have one job.”\\n“Bins,” I say, holding up two fingers.\\n“Fine, you have two jobs.”\\n“Then there’s my actual job,” I say.\\n“And when were you thinking of making a start on that?” my wife says.\\n“Right now,” I say, standing and closing my laptop.\\n“Aren’t you going to clean this up first?” she says, pointing at the puddle of piss extending under the table.\\nI say nothing, but I think: that’s four jobs.\\nOn the way to my office shed I see the tortoise on wet grass in the shadow of the house, looking glum. But when I next look up, he has disappeared.\\nI am hard at work when my wife opens my office door 20 minutes later – I saw her coming in time to put down my banjo and open my laptop.\\n“Yes?” I say, typing nonsense. “Can I help you?”\\n“Actually would you mind working in the house for a bit?” she says. “I’ve got to go out and there’s an important package about to be delivered.”\\nI stop typing and spin my chair round to face her, arms folded.\\n“How the tables have turned,” I say.\\n“Please,” she says.\\n“An important package, you say?”\\n“It’s a new radiator,” she says. “For the bathroom.”\\nWe once tried to solve this problem by installing a second wireless doorbell in my office, but it didn’t work out. It turns out many delivery people, when faced with two doorbells, will choose to knock.\\n“Fine,” I say, following my wife back to the house.\\nThe radiator arrives about 15 minutes later, but I decided not to go back to my office, because it’s warmer in the kitchen and it’s only an hour until lunch. Later that evening my wife enters the living room while I am watching the news.\\n“I’m hungry,” she says.\\n“I’ll start cooking in a minute,” I say, thinking: how many jobs is that? Five? Six?\\nAfter supper a small dispute about what to watch on TV is settled by watching what my wife wants to watch – a film she abandons three-quarters of the way through.\\n“I’m going to bed,” she says.\\n“Now?” I say.\\n“I’ve had a very busy day,” she says, watching me to see if I dare to say the same.\\n“No comment,” I say.\\nI stay to the end of the film, then turn off all the lights and head upstairs, where my wife is reading in bed. I remove one sock, then the other, while she watches me.\\n“Did you do the bins?” she says. I put my socks back on.\\nThe path to the kerb is piled with construction waste from the ripped-out bathroom, so I have to move the car out of the drive to get the bins to the street. I think: why does life have be so hard?\\nBack upstairs I get undressed, brush my teeth, climb into bed and close my eyes. A minute later, when my wife turns out her reading lamp, my eyes snap open in the dark and I think: fucking tortoise.'\n",
    "\n",
    "# with open('News.txt', encoding=\"utf8\") as f:\n",
    "#     lines = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(lines)\n",
    "\n",
    "places = []\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    if ent.label_ == \"GPE\" :\n",
    "        places.append(ent.text)\n",
    "\n",
    "print(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This blocks initialised the geography module\n",
    "from functools import partial\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"diplomacy-software\")\n",
    "\n",
    "geocode = partial(geolocator.geocode, language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries = {}\n",
    "\n",
    "for place in places:\n",
    "    try:\n",
    "        country = geocode(place)[0].split(',')[-1].strip()\n",
    "\n",
    "        if country not in countries:\n",
    "            countries[country] = 1\n",
    "        else:\n",
    "            countries[country] += 1\n",
    "\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Work\\Programming\\Hackathons\\Diplomacy software\\NLP.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work/Programming/Hackathons/Diplomacy%20software/NLP.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m total \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mlist\u001b[39m(countries\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Work/Programming/Hackathons/Diplomacy%20software/NLP.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m threshold \u001b[39m=\u001b[39m total \u001b[39m/\u001b[39;49m \u001b[39mlen\u001b[39;49m(countries)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work/Programming/Hackathons/Diplomacy%20software/NLP.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m relevant \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Work/Programming/Hackathons/Diplomacy%20software/NLP.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m country \u001b[39min\u001b[39;00m countries:\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "total = sum(list(countries.values()))\n",
    "\n",
    "threshold = total / len(countries)\n",
    "\n",
    "relevant = []\n",
    "\n",
    "for country in countries:\n",
    "    \n",
    "    if countries[country] > threshold:\n",
    "        relevant.append(country)\n",
    "\n",
    "print(relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ukraine': 14,\n",
       " 'Russia': 5,\n",
       " 'United Kingdom': 1,\n",
       " 'United States': 1,\n",
       " 'Czechia': 1,\n",
       " 'Israel': 1,\n",
       " 'Central African Republic': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
